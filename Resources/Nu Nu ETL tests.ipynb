{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidmarip/DS2002-PROJECT/blob/main/DataProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7UkPTyZkHzKS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "#importing libraries\n",
    "#https://www.geeksforgeeks.org/what-is-etl-extract-transform-load/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ujI_anrcmMpx"
   },
   "outputs": [],
   "source": [
    "def load_input_data(input_file, input_format):\n",
    "    try:\n",
    "        if input_format == 'csv':\n",
    "            return pd.read_csv(input_file)\n",
    "        elif input_format == 'json':\n",
    "            with open(input_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return pd.json_normalize(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input format. Choose 'csv' or 'json'.\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"File {input_file} not found: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "    \n",
    "#loads data either from CSV or JSON, and converts the JSON data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GnAPViA40N5U"
   },
   "outputs": [],
   "source": [
    "def modify_columns(data, keep_columns=None, add_columns=None, remove_columns=None):\n",
    "    # Check if columns exist before modifying\n",
    "    if keep_columns:\n",
    "        missing_keep = [col for col in keep_columns if col not in data.columns]\n",
    "        if missing_keep:\n",
    "            raise ValueError(f\"Columns to keep not found: {missing_keep}\")\n",
    "        data = data[keep_columns]\n",
    "\n",
    "    if add_columns:\n",
    "        for column, value in add_columns.items():\n",
    "            data[column] = value\n",
    "\n",
    "    if remove_columns:\n",
    "        missing_remove = [col for col in remove_columns if col not in data.columns]\n",
    "        if missing_remove:\n",
    "            raise ValueError(f\"Columns to remove not found: {missing_remove}\")\n",
    "        data = data.drop(columns=remove_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "# to keep, add, or drop specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Q16rEdX14m2S"
   },
   "outputs": [],
   "source": [
    "def convert_and_save(data, output_file, output_format, db_table=None, db_file=None):\n",
    "\n",
    "    if output_format == 'csv':\n",
    "        data.to_csv(output_file, index=False)\n",
    "        print(f\"Data saved as CSV to {output_file}\")\n",
    "    elif output_format == 'json':\n",
    "        data_json = data.to_json(orient='records', indent=4)\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(data_json)\n",
    "        print(f\"Data saved as JSON to {output_file}\")\n",
    "    elif output_format == 'sqlite':\n",
    "        if db_file is None or db_table is None:\n",
    "            raise ValueError(\"For SQL output, specify both db_file and db_table.\")\n",
    "        conn = sqlite3.connect(db_file)  # Use db_file, not output_file\n",
    "        try:\n",
    "            data.to_sql(db_table, conn, if_exists='replace', index=False)\n",
    "            print(f\"Data saved to SQLite table '{db_table}' in database {db_file}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported output format. Choose 'csv', 'json', or 'sqlite'.\")\n",
    "\n",
    "#convert the data format (CSV, Json, or SQL) to the desired output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "pGVRSEHp6wa4"
   },
   "outputs": [],
   "source": [
    "def store_data(data, output_format, output_file, db_table=None, db_file=None):\n",
    "  convert_and_save(data, output_file, output_format, db_table, db_file)\n",
    "\n",
    "#function to store the data in SQl or write to disk (as CSV or Json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lLfFnImREdZj"
   },
   "outputs": [],
   "source": [
    "def view_sql_data(db_file, db_table):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    query = f\"SELECT * FROM {db_table}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "17zricjmEkA6"
   },
   "outputs": [],
   "source": [
    "def etl_processor(input_file, input_format='csv', output_format='csv',\n",
    "                  output_file='output_data', keep_columns=None, add_columns=None, remove_columns=None,\n",
    "                  db_table=None, db_file=None):\n",
    "    data = load_input_data(input_file, input_format)\n",
    "    modified_data = modify_columns(data, keep_columns, add_columns, remove_columns)\n",
    "    store_data(modified_data, output_format, output_file, db_table, db_file)\n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNQuTA_vzRlP"
   },
   "source": [
    "The Input CSV file, US_Census_Tract_Area_2010.csv, is mounted through a local file. It's from Charlottesville open data with information about specific demographics. It contains 12 records and 353 columns of multiple demographic attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "28w7d49z-r2Y",
    "outputId": "2a34acb2-44ec-41af-91b5-c88f09b16409"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File Downloads/DS2002/US_Census_Tract_Area_2010.csv not found: [Errno 2] No such file or directory: 'Downloads/DS2002/US_Census_Tract_Area_2010.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mload_input_data\u001b[0;34m(input_file, input_format)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(input_file)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m         handle,\n\u001b[1;32m    875\u001b[0m         ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m         newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Downloads/DS2002/US_Census_Tract_Area_2010.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/DS2002/US_Census_Tract_Area_2010.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m#file path VARIES BY DEVICE LOCAL STORAGE!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m transformed_US2010census_data \u001b[38;5;241m=\u001b[39m etl_processor(\n\u001b[1;32m      3\u001b[0m     input_file\u001b[38;5;241m=\u001b[39minput_file,\n\u001b[1;32m      4\u001b[0m     input_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Input file format\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Desired output format ('csv', 'json', 'sql')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformed_US2010census_data\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Base name for output file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     keep_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAREA_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopulation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhite\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmIndian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHawaiian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Columns to keep\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     add_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     db_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_file\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# For SQL output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     db_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase.db\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# SQLite database for SQL output\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformed_US2010census_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m, in \u001b[0;36metl_processor\u001b[0;34m(input_file, input_format, output_format, output_file, keep_columns, add_columns, remove_columns, db_table, db_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21metl_processor\u001b[39m(input_file, input_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m, output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                   output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_data\u001b[39m\u001b[38;5;124m'\u001b[39m, keep_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, add_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                   db_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, db_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_input_data(input_file, input_format)\n\u001b[1;32m      5\u001b[0m     modified_data \u001b[38;5;241m=\u001b[39m modify_columns(data, keep_columns, add_columns, remove_columns)\n\u001b[1;32m      6\u001b[0m     store_data(modified_data, output_format, output_file, db_table, db_file)\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mload_input_data\u001b[0;34m(input_file, input_format)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported input format. Choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError decoding JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File Downloads/DS2002/US_Census_Tract_Area_2010.csv not found: [Errno 2] No such file or directory: 'Downloads/DS2002/US_Census_Tract_Area_2010.csv'"
     ]
    }
   ],
   "source": [
    "input_file = 'Downloads/DS2002/US_Census_Tract_Area_2010.csv'  #file path VARIES BY DEVICE LOCAL STORAGE!\n",
    "transformed_US2010census_data = etl_processor(\n",
    "    input_file=input_file,\n",
    "    input_format='csv',  # Input file format\n",
    "    output_format='json',  # Desired output format ('csv', 'json', 'sql')\n",
    "    output_file='transformed_US2010census_data',  # Base name for output file\n",
    "    keep_columns=['ID','AREA_','Population','White','Black', 'AmIndian', 'Asian', 'Hawaiian', 'Other'],  # Columns to keep\n",
    "    add_columns=None,\n",
    "    remove_columns=None,\n",
    "    db_table='input_file',  # For SQL output\n",
    "    db_file='database.db'  # SQLite database for SQL output\n",
    ")\n",
    "df = pd.read_json('transformed_US2010census_data')\n",
    "df.head()\n",
    "# Example Usage for US_Census_Tract_Area_2010 for Charlottesville\n",
    "# takes inputted cvs file and transforms (keeps columns ObjectID, ID, Area, Population, White, and Black)\n",
    "# output formatted as Json saved to transformed_US2010census_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BeswrXNap1X",
    "outputId": "4657e4a9-87ad-495a-d8c1-66d7260785b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-a9f5vcaiFZ"
   },
   "source": [
    "The output DataFrame is a transformed version of the US_Census_Tract_Area_2010.csv with 12 Records and only 9 columns. Of these columns includes: ID, AREA_, Population, White, Black, AmIndian, Asian, Hawaiian, Other. It focuses the data to more specific demographics and get's rid of columns such as State and County, which had the same values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNr0N1+zu0YKrEFAQosG5dG",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
